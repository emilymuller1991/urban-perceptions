
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="assets/iclogo.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.3.9">
    
    
      
        <title>Training a Convolutional Neural Network in PyTorch - Urban Perceptions</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.1d29e8d0.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.cbb835fc.min.css">
        
      
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL(".",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent="">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#training-a-convolutional-neural-network-in-pytorch" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="index.html" title="Urban Perceptions" class="md-header__button md-logo" aria-label="Urban Perceptions" data-md-component="logo">
      
  
  <svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 700 700"><defs><style>.cls-1{fill:none;}.cls-2{fill:#c7c9d0;}.cls-3{clip-path:url(#clip-path);}.cls-4{clip-path:url(#clip-path-3);}.cls-5{clip-path:url(#clip-path-4);}.cls-6{clip-path:url(#clip-path-6);}</style><clipPath id="clip-path"><rect class="cls-1" x="-227.75" y="-429.57" width="1092" height="1580"/></clipPath><clipPath id="clip-path-3"><rect class="cls-1" x="572.25" y="-429.57" width="1092" height="1580"/></clipPath><clipPath id="clip-path-4"><rect class="cls-1" x="-221" y="-430.32" width="1092" height="1580"/></clipPath><clipPath id="clip-path-6"><rect class="cls-1" x="579" y="-430.32" width="1092" height="1580"/></clipPath></defs><g id="Layer_4" data-name="Layer 4"><polygon class="cls-2" points="299.62 154.69 306.36 153.9 306.33 154.99 299.62 154.69"/><polygon class="cls-2" points="515.25 201.88 522.03 201.14 520.38 204.7 515.25 201.88"/><polygon class="cls-2" points="615.51 559.5 608.82 560.21 606.76 544.65 615.18 558.56 615.51 559.5"/><polyline class="cls-2" points="608.84 560.26 609.82 560.15 615.57 559.51 615.42 559.2 608.75 560.07"/><polyline class="cls-2" points="608.91 560.25 608.83 560.26 608.8 560.19"/></g><g id="Layer_2" data-name="Layer 2"><g class="cls-3"><g class="cls-3"><path class="cls-2" d="M352.45,423c109.1,0,204.49,55.09,256.38,137.26C593.75,421.33,484.72,313.53,352.45,313.53S111.21,421.28,96.12,560.22C148,478.05,243.35,423,352.45,423"/><path class="cls-2" d="M428.05,487.89c0,42.72-33.85,77.34-75.6,77.34s-75.59-34.62-75.59-77.34,33.84-77.33,75.59-77.33,75.6,34.62,75.6,77.33"/><polygon class="cls-2" points="237.76 383.12 162.74 430.28 29 207.61 104.02 160.44 237.76 383.12"/><rect class="cls-2" x="299.58" y="154.67" width="71.7" height="180.73"/><polygon class="cls-2" points="533.79 404.42 440.21 357.05 515.25 201.88 608.83 249.24 533.79 404.42"/></g></g><polyline class="cls-2" points="102.85 559.49 96.1 560.24 106.15 543.9"/><polygon class="cls-2" points="96.1 560.24 96.22 559.29 96.24 559.97 96.17 560.21 96.1 560.24"/></g><g class="cls-5"><g class="cls-5"><path d="M359.2,422.26c109.1,0,204.49,55.09,256.38,137.26C600.5,420.58,491.47,312.78,359.2,312.78S117.91,420.58,102.82,559.52c51.9-82.17,147.28-137.26,256.38-137.26"/><path d="M434.8,487.14c0,42.72-33.85,77.34-75.6,77.34s-75.59-34.62-75.59-77.34,33.84-77.33,75.59-77.33,75.6,34.62,75.6,77.33"/><polygon points="244.51 382.37 169.49 429.53 35.75 206.86 110.77 159.69 244.51 382.37"/><rect x="306.33" y="153.92" width="71.7" height="180.73"/><polygon points="540.54 403.67 446.96 356.3 522 201.13 615.58 248.49 540.54 403.67"/></g></g><polygon class="cls-2" points="104.04 160.45 110.76 159.69 105.41 163.05 103.94 160.58 104.04 160.45"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Urban Perceptions
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Training a Convolutional Neural Network in PyTorch
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/emilymuller1991/urban-perceptions" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="index.html" title="Urban Perceptions" class="md-nav__button md-logo" aria-label="Urban Perceptions" data-md-component="logo">
      
  
  <svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 700 700"><defs><style>.cls-1{fill:none;}.cls-2{fill:#c7c9d0;}.cls-3{clip-path:url(#clip-path);}.cls-4{clip-path:url(#clip-path-3);}.cls-5{clip-path:url(#clip-path-4);}.cls-6{clip-path:url(#clip-path-6);}</style><clipPath id="clip-path"><rect class="cls-1" x="-227.75" y="-429.57" width="1092" height="1580"/></clipPath><clipPath id="clip-path-3"><rect class="cls-1" x="572.25" y="-429.57" width="1092" height="1580"/></clipPath><clipPath id="clip-path-4"><rect class="cls-1" x="-221" y="-430.32" width="1092" height="1580"/></clipPath><clipPath id="clip-path-6"><rect class="cls-1" x="579" y="-430.32" width="1092" height="1580"/></clipPath></defs><g id="Layer_4" data-name="Layer 4"><polygon class="cls-2" points="299.62 154.69 306.36 153.9 306.33 154.99 299.62 154.69"/><polygon class="cls-2" points="515.25 201.88 522.03 201.14 520.38 204.7 515.25 201.88"/><polygon class="cls-2" points="615.51 559.5 608.82 560.21 606.76 544.65 615.18 558.56 615.51 559.5"/><polyline class="cls-2" points="608.84 560.26 609.82 560.15 615.57 559.51 615.42 559.2 608.75 560.07"/><polyline class="cls-2" points="608.91 560.25 608.83 560.26 608.8 560.19"/></g><g id="Layer_2" data-name="Layer 2"><g class="cls-3"><g class="cls-3"><path class="cls-2" d="M352.45,423c109.1,0,204.49,55.09,256.38,137.26C593.75,421.33,484.72,313.53,352.45,313.53S111.21,421.28,96.12,560.22C148,478.05,243.35,423,352.45,423"/><path class="cls-2" d="M428.05,487.89c0,42.72-33.85,77.34-75.6,77.34s-75.59-34.62-75.59-77.34,33.84-77.33,75.59-77.33,75.6,34.62,75.6,77.33"/><polygon class="cls-2" points="237.76 383.12 162.74 430.28 29 207.61 104.02 160.44 237.76 383.12"/><rect class="cls-2" x="299.58" y="154.67" width="71.7" height="180.73"/><polygon class="cls-2" points="533.79 404.42 440.21 357.05 515.25 201.88 608.83 249.24 533.79 404.42"/></g></g><polyline class="cls-2" points="102.85 559.49 96.1 560.24 106.15 543.9"/><polygon class="cls-2" points="96.1 560.24 96.22 559.29 96.24 559.97 96.17 560.21 96.1 560.24"/></g><g class="cls-5"><g class="cls-5"><path d="M359.2,422.26c109.1,0,204.49,55.09,256.38,137.26C600.5,420.58,491.47,312.78,359.2,312.78S117.91,420.58,102.82,559.52c51.9-82.17,147.28-137.26,256.38-137.26"/><path d="M434.8,487.14c0,42.72-33.85,77.34-75.6,77.34s-75.59-34.62-75.59-77.34,33.84-77.33,75.59-77.33,75.6,34.62,75.6,77.33"/><polygon points="244.51 382.37 169.49 429.53 35.75 206.86 110.77 159.69 244.51 382.37"/><rect x="306.33" y="153.92" width="71.7" height="180.73"/><polygon points="540.54 403.67 446.96 356.3 522 201.13 615.58 248.49 540.54 403.67"/></g></g><polygon class="cls-2" points="104.04 160.45 110.76 159.69 105.41 163.05 103.94 160.58 104.04 160.45"/></svg>

    </a>
    Urban Perceptions
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/emilymuller1991/urban-perceptions" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="index.html" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="1.download_images.html" class="md-nav__link">
        How To Download Google Street View Images
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="2.web_app.html" class="md-nav__link">
        Creating and Deploying a Web-App
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Training a Convolutional Neural Network in PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="3.deep_cnn.html" class="md-nav__link md-nav__link--active">
        Training a Convolutional Neural Network in PyTorch
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#training-locally" class="md-nav__link">
    Training Locally
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hyperparameter-optimisation" class="md-nav__link">
    Hyperparameter Optimisation
  </a>
  
    <nav class="md-nav" aria-label="Hyperparameter Optimisation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hyperparameter-optimisation-using-wandbai" class="md-nav__link">
    Hyperparameter optimisation using wandb.ai
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dataset" class="md-nav__link">
    Dataset
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#training-locally" class="md-nav__link">
    Training Locally
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hyperparameter-optimisation" class="md-nav__link">
    Hyperparameter Optimisation
  </a>
  
    <nav class="md-nav" aria-label="Hyperparameter Optimisation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hyperparameter-optimisation-using-wandbai" class="md-nav__link">
    Hyperparameter optimisation using wandb.ai
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dataset" class="md-nav__link">
    Dataset
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
  <a href="https://github.com/emilymuller1991/urban-perceptions/tree/main/docs/3.deep_cnn.md" title="Edit this page" class="md-content__button md-icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


  



<h1 id="training-a-convolutional-neural-network-in-pytorch">Training a Convolutional Neural Network in PyTorch</h1>
<p>In this section we will fine-tune a pretrained CNN to learn to predict perception scores. Before we dive into the components of the model, we will just get the code up and running with the default settings.</p>
<p><img alt="scatterplot of ground truth versus predicted scores" src="images/pp_mse.PNG" /></p>
<h2 id="training-locally">Training Locally</h2>
<p>If you have not yet done so, now is a good time to clone this repository into your local drive (see Getting Started).</p>
<p>Let's check the model training runs locally (albeit slowly without a GPU). From urban-perceptions run:</p>
<div class="highlight"><pre><span></span><code>python3 -m deep_cnn                     <span class="se">\</span>
--epochs<span class="o">=</span><span class="m">1</span>                              <span class="se">\</span>
--data_dir<span class="o">=</span>tests/tests3/test_input      <span class="se">\</span>
--wandb<span class="o">=</span>False                           <span class="se">\</span>
</code></pre></div>
<p>A brief description of all arguments is returned with the following:</p>
<div class="highlight"><pre><span></span><code>python3 -m deep_cnn -h
usage: __main__.py <span class="o">[</span>-h<span class="o">]</span> <span class="o">[</span>--epochs EPOCHS<span class="o">]</span> <span class="o">[</span>--batch_size BATCH_SIZE<span class="o">]</span>
                   <span class="o">[</span>--model MODEL<span class="o">]</span> <span class="o">[</span>--pre PRE<span class="o">]</span> <span class="o">[</span>--study_id STUDY_ID<span class="o">]</span>
                   <span class="o">[</span>--oversample OVERSAMPLE<span class="o">]</span> <span class="o">[</span>--root_dir ROOT_DIR<span class="o">]</span> <span class="o">[</span>--lr LR<span class="o">]</span>
                   <span class="o">[</span>--run_name RUN_NAME<span class="o">]</span> <span class="o">[</span>--data_dir DATA_DIR<span class="o">]</span> <span class="o">[</span>--wandb WANDB<span class="o">]</span>

optional arguments:
  -h, --help            show this <span class="nb">help</span> message and <span class="nb">exit</span>
  --epochs EPOCHS       number of training epochs
  --batch_size BATCH_SIZE
                        batch size <span class="k">for</span> SGD
  --model MODEL         pre-trained model
  --pre PRE             pre-processing <span class="k">for</span> image input
  --study_id STUDY_ID   perceptions from place pulse study
  --oversample OVERSAMPLE
                        whether to oversample
  --root_dir ROOT_DIR   path to recode-perceptions
  --lr LR               learning rate
  --run_name RUN_NAME   unique name to identify hyperparameter choices
  --data_dir DATA_DIR   path to input data
  --wandb WANDB         track progress <span class="k">in</span> wandb.ai
</code></pre></div>
<p>where <code>root_dir</code> is your local path to urban-perceptions, and <code>data_dir</code> is your path to <code>input/pp_images</code>. If you have not yet downloaded the full image dataset, but you want to run this script locally, you can run the test images, tests/test_input/test_images/.</p>
<p>You should see the following output in your terminal:</p>
<div class="highlight"><pre><span></span><code>Running on cuda device
Epoch 0:   2%|██▎                                 | 35/1697 [00:19&lt;15:28,  1.79batch/s, loss=3.06]
</code></pre></div>
<p>If you have a GPU locally, you will also see 'Running on <code>cuda</code> device'. However, this will be replaced by CPU if no GPU device is found. The model is training through its first epoch batch by batch. This one epoch is expected to take 15 minutes to complete.</p>
<p>Once finished, the full log can be found in the folder outputs/logger/.</p>
<h2 id="hyperparameter-optimisation">Hyperparameter Optimisation</h2>
<p>Let's take a look at the design choices which can be made for model training.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
<th>Trade Offs</th>
<th>References</th>
</tr>
</thead>
<tbody>
<tr>
<td>Epochs</td>
<td>Determine the number of times to run through the entire training batch.</td>
<td>Typically there is an inflection point where decreases in loss are marginal. Continued increase after this reflects overfitting</td>
<td><a href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html">Bias-Variance Trade-Off</a></td>
</tr>
<tr>
<td>Batch size</td>
<td>Number of samples for each training step</td>
<td>Smaller batch sizes increase performance of stochastic gradient descent algorithms while also preserving memory by loading in batches. Larger batch size can speed up computation.</td>
<td><a href="https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu">Batch-size</a></td>
</tr>
<tr>
<td>Learning Rate</td>
<td>Step size of parameter update at each iteration</td>
<td>A larger learning rate requires fewer epochs but can easily result in unstable results such as local minima. Smaller learning rates can fail to find an optima at all. Adaptive learning rates consider large step sizes in the earlier epochs of training and reduces the step size in later epochs</td>
<td><a href="https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/">Learning Rates</a></td>
</tr>
<tr>
<td>Model</td>
<td>Pre-trained model</td>
<td>Can vary by size, depth, structure and trainable parameters Smaller models are faster to train while deeper model typically achieve higher levels of abstraction. Models with dropout can avoid overfitting,</td>
<td><a href="https://pytorch.org/vision/stable/models.html">PyTorch pre-trained models</a></td>
</tr>
<tr>
<td>Pre-processing</td>
<td>Image pre-processing required for model input</td>
<td>Pre-trained models have parameters trained within a given range and perform better when the target dataset distribution is closer matched to the source dataset distribution</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>There is never a one-rule-fits-all-approach to training a deep neural network. Design choices can be guided by the domain task, the dataset size and distribution, hardware constraints, time constraints or all of the above. The wonder of moden software and computing is that the cycle of iterating on model choices has been sped up enormously:</p>
<p><img alt="alt text" src="images/image_tasks.jpeg" title="Hyperparameter Optimisation Cycle" /></p>
<p>There exists many types of <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">search algorithms</a> for finding the optimal hyperparameters, such as gridded search, random search and Bayesian optimisation. We would like to add a priori the wisdom of the crowd. There has been a lot written about deep learning hyperparameters, so we don't need to go in blind. To help you configure some intervals, consider the following questions</p>
<ol>
<li>How many epochs will ensure you have reached the global minima?</li>
<li>How much memory constraints do you have on model and batch size?</li>
<li>What ratio of batch size to data samples/classes is considered a benchmark?</li>
<li>What is a typical learning rate for similar image classification tasks?</li>
<li>How can an adaptive learning rate be implemented?</li>
<li>Which model has shown the best performance on benchmark datasets? Is there a PyTorch version to download pre-trained weights?</li>
<li>Does this model require certain pre-processing? If yes, you will have to add the preprocessing to dataset_generator.py</li>
</ol>
<p>Once you have tried answered the questions above, you will have a constraint on what is reasonable to test.</p>
<p>When performing hyperparameter optimisation, we will not evaluate our test performance during training. This test set will be a hold-out set used only to evaluate the performance of our final model after training. Instead, we evaluate the performance of the model during training on the validation set. This is done typically to avoid overfitting to the test set during hyperparameter optimisation. Our goal in training is to test the generalisability of our training paradigm and the test-set allows us to do this.</p>
<h3 id="hyperparameter-optimisation-using-wandbai">Hyperparameter optimisation using <code>wandb.ai</code></h3>
<p>We will use weights and biases to track our model training and validation. This platform uses a lightweight python package to log metrics during training. To use this, you will need to create an account at <a href="https://wandb.ai/site">https://wandb.ai/site</a>. You can create an account using your GitHub or Gmail. You will be prompted to create a username, keep a note of this. Once you have completed your registration, you will be directed to a landing page with an API key, keep a note of this too.</p>
<p>Once you have configured your user credentials, create a new project called recode-perceptions. This folder will log all of our training runs. In order for python to get access to your personal user credentials, you will have to set them as environmental variables which python then accesses using <code>os.getenv("WB_KEY")</code>. Set your environmental variables as follows:</p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span> <span class="nv">WB_KEY</span><span class="o">=</span>API_KEY
<span class="nb">export</span> <span class="nv">WB_PROJECT</span><span class="o">=</span><span class="s2">&quot;urban-perceptions&quot;</span>
<span class="nb">export</span> <span class="nv">WB_USER</span><span class="o">=</span><span class="s2">&quot;username&quot;</span>
</code></pre></div>
<p>If you now run the scripts with <code>--wandb=True</code>, you should begin to see the metrics being tracked on the platform:</p>
<h3 id="dataset">Dataset</h3>
<p>The Place Pulse dataset can be downloaded by executing the data_download.sh script. You will first need to enter your remote path to the urban-perceptions directory (line 5).</p>
<p>The datasets were not released by us, and we do not claim any rights on them. Use the datasets at your responsibility and make sure you fulfil the licenses that they were released with. If you use any of the datasets please consider citing the original authors of <a href="https://www.media.mit.edu/projects/place-pulse-new/overview/">Place Pulse MIT</a>.</p>
<p>Revisit Hyperparameter Optimisation and iterate on model choices using run_name to track design choices.</p>

              
            </article>
            
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="2.web_app.html" class="md-footer__link md-footer__link--prev" aria-label="Previous: Creating and Deploying a Web-App" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Creating and Deploying a Web-App
            </div>
          </div>
        </a>
      
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": ".", "features": ["navigation.instant", "navigation.top", "toc.follow", "content.code.annotate"], "search": "assets/javascripts/workers/search.b97dbffb.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="assets/javascripts/bundle.6c7ad80a.min.js"></script>
      
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
      
    
  </body>
</html>